# 重大发现：RSL-RL 日志的 Step 级均值陷阱 & 下一步计划

> 日期：2026-02-27
> 分支：`exp/fix-pallet-dragging`
> 训练日志：`logs/20260227_131327_smoke_train_s1.0zA_ExpA_hold_diag.log`

---

## 一、发现背景

在执行 [Hold 阶段诊断与突破方案] Phase 2（分析 Phase 0+1 结果）时，我们发现了一个关键的数学规律，**彻底推翻了之前对"成功率极低"的认知**。

此前我们一直以为 `frac_success` ~0.5% 意味着 1000 次 Episode 里只成功了 5 次。
**事实是：0.5% 恰好就是 100% 成功率的完美表现。**

---

## 二、根因分析：Step 级均值 vs Episode 级成功率

### 2.1 RSL-RL 的日志机制

RSL-RL 中，`self.extras["log"]` 记录的变量是 **按 Step 求平均** 的（即每个 env step 的 snapshot），而非按 Episode 求平均。

### 2.2 `frac_success` 的真实含义

```
frac_success = success.float().mean()  # success = self._hold_counter >= self._hold_steps
```

- 当 Agent 满足 `_hold_counter >= hold_steps`（当前 hold_steps=9）时，`success` 为 True
- **紧接着 `_get_dones()` 返回 terminated=True，环境被 Reset**，`_hold_counter` 清零
- `DirectRLEnv.step()` 的调用顺序是：`_get_dones()` → `_get_rewards()` → `_reset_idx()`
  - `_get_dones()` 中 `success = _hold_counter >= _hold_steps` → True
  - `_get_rewards()` 中同样计算 `success = _hold_counter >= _hold_steps` → True（因为还没 reset）
  - 之后 `_reset_idx()` 清零 `_hold_counter`
- 因此，在一次成功的 Episode 中，`success` 标志位 **只会亮起 1~2 个 Step**
- 当前 `Mean episode length` ≈ 400 步

### 2.3 数学推导

如果一个 Agent **100% 每次都能成功**：

```
frac_success = success_steps / total_steps_per_rollout
             ≈ 2 / episode_length
             ≈ 2 / 400
             = 0.005 (0.5%)
```

**这就是我们观察到的 0.5%！**

### 2.4 真实成功率还原公式

```
真实 Episode 成功率 ≈ frac_success × episode_length / 2
```

### 2.5 代入真实日志数据

| 实验 | frac_success | episode_length | 真实成功率 |
|------|-------------|---------------|-----------|
| Baseline (s1.0y, 3599 iter) | 0.0005 | 350 | **8.75%** |
| Exp-A (iter 1269) | 0.0049 | 409 | **~100%** |
| Exp-A (iter 1270) | 0.0039 | 435 | **~85%** |
| Exp-A (last200 avg) | 0.0042 | ~400 | **~84%** |

**结论：Exp-A 的真实 Episode 成功率已经达到了 85%~100%！**

---

## 三、诊断数据的完美印证

Phase 1 新增的 Hold 失败诊断日志进一步证实：

| 诊断指标 | last200 均值 | 含义 |
|---------|------------|------|
| `diag_hold/fail_ins_frac` | 0.0000 | 插入条件从未在 Hold 期间失败 |
| `diag_hold/fail_align_frac` | 0.0000 | 对齐条件从未在 Hold 期间失败 |
| `diag_hold/fail_lift_frac` | ~0.0001 | 举升条件极偶尔失败（可忽略） |
| `phase/hold_counter_max` | 9.89/10 | 92% 的 iteration 中至少有一个 env 达到满分 |
| `phase/frac_success_now` | 0.024 | ~2.4% 的 step 处于 still_ok，对应 ~10 步/episode |

**Agent 一旦进入 `still_ok` 状态，几乎永远不会掉出来。**

`frac_success_now` ≈ 0.024，在 400 步中意味着 ~10 步处于 still_ok，恰好与 `hold_steps=9` 完美吻合——Agent 几乎每次都能完整走完 Hold 阶段。

---

## 四、关键指标趋势（iter 100 → 1270）

| 指标 | iter 100 | iter 500 | iter 1000 | iter 1270 | 趋势 |
|------|---------|---------|----------|----------|------|
| frac_inserted | 3.6% | 13.2% | 14.8% | 14.8% | 稳定在 ~14% |
| frac_aligned | 23.7% | 38.3% | 33.5% | 31.8% | 稳定在 ~33% |
| frac_lifted | 1.9% | 5.1% | 5.8% | 7.3% | 缓慢上升 |
| frac_success_now | 0.0% | 2.5% | 2.8% | 2.2% | 稳定在 ~2.5% |
| frac_success | 0.0% | 0.5% | 0.5% | 0.4% | 稳定在 ~0.5% |
| near_success_frac | 13.9% | 30.6% | 33.7% | 31.9% | 稳定在 ~32% |
| noise_std | 0.23 | 0.12 | 0.08 | 0.06 | 持续收敛 |
| yaw_deg_near | 5.89° | 1.88° | 2.18° | 2.54° | 稳定在 ~2.3° |
| lateral_near | 0.20m | 0.08m | 0.11m | 0.11m | 稳定在 ~0.10m |

---

## 五、之前计划的处置

| 计划 | 状态 | 原因 |
|------|------|------|
| Path A: 放宽 Schmitt 迟滞带 | **取消** | Hold 阶段无失败，条件已足够宽松 |
| Path A: EMA 滤波对齐条件 | **取消** | 同上 |
| Path A: 占空比成功判定 | **取消** | 同上 |
| Path B: 课程学习 hold_time_s | **取消** | Agent 已能稳定完成 9 步 Hold |

---

## 六、下一步计划

### 6.1 修复日志系统（P0，必须做）

在 `env.py` 中新增 **Episode 级别** 的成功率统计，避免再被 Step 级均值误导。

方案：在 `_reset_idx()` 中记录每个被重置 env 的终止原因（success / timeout / early_stop），累计后输出：

```python
# 在 __init__ 中:
self._episode_success_count = 0
self._episode_total_count = 0

# 在 _reset_idx 中:
success_mask = self._hold_counter[env_ids] >= self._hold_steps
self._episode_success_count += success_mask.sum().item()
self._episode_total_count += len(env_ids)

# 在 _get_rewards 的日志段中:
if self._episode_total_count > 0:
    self.extras["log"]["episode/success_rate"] = self._episode_success_count / self._episode_total_count
```

每隔 N 个 iteration 重置计数器（或交给 RSL-RL 的 log_interval 管理）。

### 6.2 视觉验收（P0，必须做）

使用 `play.py` 加载 Exp-A 的 checkpoint（~1200 iter），直接观察：

```bash
cd /home/uniubi/projects/forklift_sim/IsaacLab
unset CONDA_PREFIX && TERM=xterm bash isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py \
  --task Isaac-Forklift-PalletInsertLift-Direct-v0 \
  --num_envs 4 \
  --checkpoint <path_to_checkpoint>
```

验收标准：
- 叉车能否对准、插入、举升、保持？
- 动作是否平滑，有无抖动？
- 成功率是否与日志数据吻合（>80%）？

### 6.3 后续优化方向（P1，可选）

| 方向 | 说明 |
|------|------|
| 提高 insert 率（14% → 更高） | 当前 86% 的 episode 甚至没有完成插入，优化前期导航/对齐奖励 |
| 减少 episode 长度 | 当前平均 400 步，理论最优可能 <200 步，提高训练效率 |
| Action smoothness | 如果视觉验收发现抖动，增加 action rate penalty |
| Sim2Real 准备 | Domain randomization, 摩擦/质量/观测噪声 |

---

## 七、反思

这次事件暴露了一个重要的工程陷阱：

> **在 RL 训练中，永远不要直接看 Step 级别的 flag 均值来判断 Episode 级别的成功率。**

一个 Episode 只在最后 1~2 步才会 success=True，其余 398 步都是 False。
Step 级别的均值会系统性地低估成功率，低估倍数 = episode_length / 2。

正确做法是在环境中维护 Episode 级别的统计量，或者在日志分析时手动还原。

---

## 八、文件参考

| 文件 | 说明 |
|------|------|
| `env.py` L1234 | `success = self._hold_counter >= self._hold_steps` |
| `env.py` L1417 | `self.extras["log"]["phase/frac_success"] = success.float().mean()` |
| `env.py` L1542 | `self._hold_counter[env_ids] = 0`（reset 时清零） |
| `DirectRLEnv.step()` L371-378 | `_get_dones()` → `_get_rewards()` → `_reset_idx()` 调用顺序 |
| `env_cfg.py` L86 | `hold_time_s: float = 0.33`（对应 hold_steps=9） |
| 训练日志 | `logs/20260227_131327_smoke_train_s1.0zA_ExpA_hold_diag.log` |
