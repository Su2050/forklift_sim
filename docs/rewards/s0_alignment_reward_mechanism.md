# S0 对齐奖励机制详解

> 本文档详细解析 S0/S0.2 版本中对齐奖励机制的设计原理、实现细节和训练效果。

---

## 一、核心设计理念

S0 的对齐奖励策略基于**"对齐优先"**原则：

- **阶段A：对齐为王**（减小横向误差 + 偏航误差）
- **阶段B：对齐好了才给接近/插入/抬升奖励**

### 设计目标

1. **避免"站桩刷分"**：使用增量型奖励，只奖励改善，不奖励维持
2. **平滑过渡**：使用软门控权重，避免硬切换导致的奖励跳变
3. **严格门控**：通过乘法门控确保横向和偏航都达标
4. **防止斜怼**：未对齐时插入会被惩罚

---

## 二、对齐误差计算：`E_align`

### 2.1 误差分量

```python
# 横向误差（左右偏移）
y_err = |pallet_pos_y - robot_pos_y|  # 单位：米

# 偏航误差（朝向差）
yaw_err_deg = |pallet_yaw - robot_yaw|  # 单位：度
```

### 2.2 归一化综合误差

```python
E_align = y_err / lat_ready_m + yaw_err_deg / yaw_ready_deg

# 其中：
# lat_ready_m = 0.10m（对齐就绪横向阈值）
# yaw_ready_deg = 10.0°（对齐就绪偏航阈值）
```

**设计要点**：
- **归一化**：将不同量纲的误差统一到同一尺度
- **相加**：横向和偏航误差同等重要
- **无单位**：`E_align` 是纯数值，便于比较和计算

### 2.3 数值示例

| 情况 | y_err | yaw_err | E_align | 评价 |
|------|-------|---------|---------|------|
| 完美对齐 | 0.00m | 0° | 0.00 | 理想状态 |
| 良好对齐 | 0.05m | 5° | 0.5 + 0.5 = **1.0** | 可接受 |
| 中等对齐 | 0.08m | 8° | 0.8 + 0.8 = **1.6** | 需改善 |
| 较差对齐 | 0.15m | 15° | 1.5 + 1.5 = **3.0** | 很差 |
| 初始状态 | 0.20m | 7.7° | 2.0 + 0.77 = **2.77** | 训练起点 |

---

## 三、软门控权重：`w_ready`

### 3.1 权重计算公式

```python
# 横向对齐权重
w_lat = clamp(1.0 - y_err / lat_ready_m, 0, 1)
# y_err=0时，w_lat=1；y_err≥0.1m时，w_lat=0

# 偏航对齐权重
w_yaw = clamp(1.0 - yaw_err_deg / yaw_ready_deg, 0, 1)
# yaw_err=0°时，w_yaw=1；yaw_err≥10°时，w_yaw=0

# 综合对齐就绪权重（乘法门控）
w_ready = w_lat * w_yaw
```

### 3.2 乘法门控的含义

**关键特性**：
- **任一维度不达标，`w_ready` 都会降低**
- **必须同时满足横向和偏航要求，`w_ready` 才会接近1**

**为什么用乘法而不是加法？**
- 乘法确保**两个条件都必须满足**（AND逻辑）
- 加法会导致"一个好就能补偿另一个差"（OR逻辑）

### 3.3 数值示例

| y_err | yaw_err | w_lat | w_yaw | w_ready | 说明 |
|-------|---------|-------|-------|---------|------|
| 0.00m | 0° | 1.0 | 1.0 | **1.0** | 完美对齐 |
| 0.05m | 5° | 0.5 | 0.5 | **0.25** | 中等对齐 |
| 0.10m | 0° | 0.0 | 1.0 | **0.0** | 横向超标 |
| 0.00m | 10° | 1.0 | 0.0 | **0.0** | 偏航超标 |
| 0.08m | 8° | 0.2 | 0.2 | **0.04** | 较差对齐 |

---

## 四、对齐奖励：`r_align`

### 4.1 增量型奖励公式

```python
# 对齐进步奖励（阶段 A 核心）
r_align = k_align * (last_E_align - E_align)
# k_align = 1.0
```

### 4.2 增量型设计原理

**为什么用增量而不是绝对值？**

1. **避免"站桩刷分"**：
   - 如果奖励 `E_align` 的绝对值，策略会学会"停在某个位置不动"
   - 增量型只奖励"改善"，必须持续改善才能获得奖励

2. **提供方向信号**：
   - `delta_E_align > 0` → 改善 → 奖励
   - `delta_E_align < 0` → 恶化 → 惩罚
   - `delta_E_align = 0` → 维持 → 无奖励

### 4.3 数值示例

假设 `k_align = 1.0`：

| 步骤 | last_E_align | E_align | delta_E_align | r_align | 说明 |
|------|-------------|---------|---------------|---------|------|
| t=0 | 2.77 | 2.77 | 0.00 | 0.00 | 初始化（首步保护） |
| t=1 | 2.77 | 2.50 | +0.27 | **+0.27** | 改善，奖励 |
| t=2 | 2.50 | 2.30 | +0.20 | **+0.20** | 继续改善 |
| t=3 | 2.30 | 2.30 | 0.00 | 0.00 | 维持，无奖励 |
| t=4 | 2.30 | 2.50 | -0.20 | **-0.20** | 恶化，惩罚 |
| t=5 | 2.50 | 1.00 | +1.50 | **+1.50** | 大幅改善 |

### 4.4 首步保护

```python
# 首步不给增量奖励（避免初始化跳变）
r_align = torch.where(self._is_first_step, torch.zeros_like(r_align), r_align)
```

**原因**：reset 时用当前状态初始化缓存，首步的 delta 可能异常大。

---

## 五、对齐奖励与其他奖励的交互

### 5.1 门控其他奖励

```python
# 接近奖励：对齐好了才给
r_approach = k_approach * w_ready * clamp(delta_dist, -0.05, 0.05)
# k_approach = 12.0 (S0.2)

# 插入进度奖励：对齐好了才给
r_insert = k_insert * w_ready * clamp(delta_insert, -0.03, 0.03)
# k_insert = 10.0
```

**效果**：
- `w_ready ≈ 0` 时，接近/插入奖励被抑制到接近0
- 未对齐时无法通过接近/插入获得奖励

### 5.2 斜怼惩罚

```python
# 未对齐插入惩罚（防斜怼）
pen_wrong_insert = -k_wrongins * (1.0 - w_ready) * relu(delta_insert)
# k_wrongins = 6.0
```

**效果**：
- `w_ready ≈ 0` 时，插入会被惩罚（`pen_wrong_insert ≈ -6.0 * delta_insert`）
- `w_ready ≈ 1` 时，惩罚消失（`pen_wrong_insert ≈ 0`）

### 5.3 贴脸惩罚

```python
# 安全距离惩罚（没对齐别贴脸）
pen_close = -k_close * (1.0 - w_ready) * relu(d_safe_m - dist_front)
# k_close = 2.0
# d_safe_m = 0.70m
```

**效果**：
- 未对齐且距离过近时惩罚
- 防止未对齐就贴脸

---

## 六、完整奖励流程

```python
# Step 1: 计算对齐误差
y_err = |pallet_y - robot_y|
yaw_err_deg = |pallet_yaw - robot_yaw|
E_align = y_err / 0.10 + yaw_err_deg / 10.0

# Step 2: 计算软门控权重
w_lat = clamp(1.0 - y_err / 0.10, 0, 1)
w_yaw = clamp(1.0 - yaw_err_deg / 10.0, 0, 1)
w_ready = w_lat * w_yaw

# Step 3: 计算对齐奖励（增量型）
delta_E_align = last_E_align - E_align
r_align = 1.0 * delta_E_align  # k_align = 1.0

# Step 4: 更新缓存
last_E_align = E_align

# Step 5: 门控其他奖励
r_approach = 12.0 * w_ready * delta_dist
r_insert = 10.0 * w_ready * delta_insert
pen_wrong_insert = -6.0 * (1.0 - w_ready) * relu(delta_insert)
```

---

## 七、训练效果分析

### 7.1 对齐学习成功

| 指标 | 初始值 | 最终值 | 改善幅度 |
|------|--------|--------|----------|
| **E_align** | 2.81 | 0.56 | **-80%** ✅ |
| **w_ready** | 0.04 | 0.66 | **+1550%** ✅ |
| **lateral** | 0.20m | 0.037m | **-82%** ✅ |
| **yaw_deg** | 7.7° | 1.86° | **-76%** ✅ |
| **frac_aligned** | 1.4% | 64.8% | **+63%** ✅ |

### 7.2 奖励分量（最终状态）

| 分量 | 值 | 说明 |
|------|-----|------|
| `r_align` | 0.0024 | 对齐进步奖励（很小） |
| `r_approach` | 0.0091 | 接近奖励（很小） |
| `r_insert` | 0.0000 | 插入奖励（未触发） |

### 7.3 问题诊断

**策略陷入"远处对齐躺平"的局部最优**：

1. **完全没有学会前进**
   - `dist_front_mean` 始终在 2.4-2.5m（初始位置）
   - `rew/r_approach` 只有 0.0091（太小，不足以推动前进）

2. **完全没有学会插入**
   - `insert_norm_mean` 始终为 0
   - `frac_inserted` 始终为 0%

**根因分析**：
- ✅ `r_align` 在远处也能拿到（对齐变好就有奖励）
- ⚠️ `r_approach` 需要 `w_ready * delta_dist`，但策略发现"不动也不会被惩罚"
- ⚠️ `time_penalty` 只有 -0.001/step，不足以推动前进
- ⚠️ `k_approach=12.0` 仍然不够大，无法克服"不动"的惯性

---

## 八、设计优缺点

### 8.1 优点

1. **增量型奖励避免"站桩刷分"**
   - 只奖励改善，不奖励维持
   - 策略必须持续改善才能获得奖励

2. **软门控实现平滑过渡**
   - 避免硬切换导致的奖励跳变
   - 策略可以逐步学习对齐

3. **乘法门控确保严格对齐**
   - 横向和偏航都必须达标
   - 防止"一个好就能补偿另一个差"

4. **门控机制防止未对齐时获得其他奖励**
   - 未对齐时接近/插入奖励被抑制
   - 防止策略学会"斜怼"

### 8.2 缺点

1. **稳态下梯度为零**
   - 维持坏状态时无惩罚
   - 导致"对齐但不前进"的局部最优

2. **对齐奖励信号过弱**
   - `r_align ≈ 0.0024`，容易被其他信号淹没
   - 无法有效引导策略对齐

3. **门控过严**
   - `lat_ready_m=0.1m`, `yaw_ready_deg=10°` 过严格
   - 导致策略不敢前进

---

## 九、与后续版本对比

| 特性 | S0 | S1.0c |
|------|-----|-------|
| **对齐奖励类型** | 纯增量 | 增量 + 绝对惩罚 |
| **对齐奖励公式** | `r_align = k_align * delta_E_align` | `r_align = k_align * delta_E_align`<br>`pen_align_abs = -k_align_abs * w_close * E_align` |
| **门控机制** | `w_ready` 门控接近/插入 | `w_ready` 门控插入，接近无条件 |
| **对齐阈值** | 严格（0.1m, 10°） | 宽松（0.5m, 30°） |
| **稳态问题** | ❌ 存在（梯度为零） | ✅ 解决（绝对惩罚） |

---

## 十、参数配置

### 10.1 对齐阈值参数

| 参数 | 值 | 说明 |
|------|-----|------|
| `lat_ready_m` | 0.10m | 对齐就绪横向阈值（严格） |
| `yaw_ready_deg` | 10.0° | 对齐就绪偏航阈值（严格） |
| `d_safe_m` | 0.70m | 安全站位距离 |

### 10.2 奖励系数

| 参数 | 值 | 说明 |
|------|-----|------|
| `k_align` | 1.0 | 对齐进步奖励系数 |
| `k_close` | 2.0 | 贴脸惩罚系数 |
| `k_wrongins` | 6.0 | 斜怼惩罚系数 |

---

## 十一、总结

S0 的对齐奖励策略通过：

1. **归一化综合误差 `E_align`** 量化对齐质量
2. **软门控权重 `w_ready`** 实现平滑过渡
3. **增量型奖励 `r_align`** 引导对齐改善
4. **门控机制** 防止未对齐时获得其他奖励

**效果**：
- ✅ **对齐学习非常成功**（E_align 降低80%）
- ❌ **但存在"对齐但不前进"的局部最优问题**

**后续版本改进**：
- S1.0c 通过添加绝对惩罚和调整门控策略来解决稳态问题
- 放宽对齐阈值，让策略更容易前进

---

## 十二、相关文档

- [S0 基础方案](./reward_function_s0.md)
- [S0.2 训练结果](./reward_function_s0_2_2026-02-03_15-40-50.md)
- [奖励函数演进](./reward_function.md)

---

*文档创建时间：2026-02-07*  
*基于 S0/S0.2 版本实现*
