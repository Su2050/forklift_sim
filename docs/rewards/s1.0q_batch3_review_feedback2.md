根据你们 2026-02-13 的 S1.0Q 文档里对 Batch-3 的描述，**stuck detector** 本质上是一个“**卡死自动判负/提前重开**”的机制：当策略进入你们定义的“深插死区”，并且状态在一段时间里几乎不再变化（典型是被碰撞顶住、怎么打方向都不动），就**判定这一局已经“挂在墙上”**，直接给惩罚并提前结束 episode。这样能把最耗算力的长尾失败（跑满 max step）砍掉。

---

## 1) 它要解决什么问题？

你们前面已经发现：大量失败 episode 会在 **`insert_norm` 很深、`y_err` 很大**的区域卡住，一直磨到 timeout（1079 steps 那种），吞掉大量训练步数并污染 step 口径指标。这一类失败在 Batch-3 被明确叫做 **“死区 timeout”**。

所以 stuck detector 的目标不是“把策略变聪明”（那是 reward shaping/观测的问题），而是更像工程里的**看门狗（watchdog）**：

> “你已经在坏状态里长时间没有进展了，那我就不让你继续浪费算力。”

---

## 2) 它怎么判定“卡死”？

### 2.1 先限定一个区域：死区（dead_zone）

你们的死区定义在计划里写得很明确：

* `insert_norm > 0.30` 且 `y_err > 0.20`

这很关键：因为**在非死区里“慢”不一定是坏事**（可能是在细调），但在死区里“深插 + 大横偏”通常就是撞墙。

### 2.2 再看“有没有进展”：状态变化是否接近 0

Batch-3 里 stuck detector 的检测条件写得很具体：

* `insert_norm` 的变化 < 0.005
* **且** `y_err` 的变化 < 0.005
* 连续满足 **30 步**触发（`dz_stuck_steps=30`）

这其实是在问一个很朴素的问题：

> “你在死区里连续 30 步都没把深度/横向往正确方向挪动，那就别演了。”

---

## 3) 触发后做什么？

Batch-3 文档写的是 **“penalty + early done 联合作用”**：

* 触发后给一个 **-2.0 的 penalty**
* 同时 **立即终止 episode 并重置**，避免继续跑满 max step

用伪代码讲就是：

```python
dead_zone = (insert_norm > 0.30) and (y_err > 0.20)

if dead_zone and abs(insert_norm - prev_insert_norm) < 0.005 and abs(y_err - prev_y_err) < 0.005:
    stuck_counter += 1
else:
    stuck_counter = 0

if stuck_counter >= dz_stuck_steps:  # 30
    reward += -2.0
    done = True  # early terminate
```

---

## 4) 这个机制为什么“威力巨大”，同时也为什么“容易伤到自己”？

### 4.1 威力：它几乎清空了死区 timeout

Batch-3 里最硬的证据是“失败模式拆分”：

* baseline：**死区 timeout 339**，非死区 timeout 205
* stuck detector：**死区 timeout 5**，非死区 timeout 281
  也就是说它对“碰撞卡死型”失败几乎是定点爆破（-98.5%）。

同时整体指标上：

* timeout_frac：14.62% → 5.75%
* fail_step_share：35.94% → 26.43%
* mean_fail_ep_len：1022 → 398
  这些都是因为**把“磨到死”的失败局切断了**。

### 4.2 代价：30 步太激进，误杀可恢复局 + 引发 churn

同一份文档也写得很直白：

* success_rate：84.28% → 76.57%（显著下降）
* lateral_near_p90：0.306 → 0.415m（显著恶化）
* P_ever_dead_zone：11.63% → 19.55%（上升，churn）

原因里有两条你们写得很关键：

1. **30 步≈1.5 秒模拟时间**，在死区里 1.5 秒“不动”，可能只是“在犹豫/探索”，未必真不可恢复，所以会误杀一批本来能成功的 episode。

2. **churn 效应**：提前终止释放了 env slot，导致产生更多 episode（4977 vs 3722），并可能形成“终止→重置→又进死区→又终止”的循环，让某些统计量（比如死区发生率、lateral 分布）变得更难解释。文档里明确点到了这一点。

---

## 5) 用一句更“控制论”的话总结它

* **reward shaping** 是在教策略“怎么做更好”；
* **stuck detector** 更像是在改环境规则：
  “某些状态/轨迹太浪费资源，直接判负/重开。”

它会改变训练分布和最优策略（因为 agent 会学会避开触发终止的状态），所以它的威力与副作用永远绑定在一起。

---

## 6) 为什么 Batch-4 要扫 `dz_stuck_steps=60/80/100`？

你们 Batch-3 的结论已经写出来了：**机制对，阈值过激**，下一步就是找“减少 timeout”与“保护 success”之间的平衡点，优先扫 60/80/100。

直觉上：

* 步数阈值越大 → **误杀率越低**（更宽容）→ success 更容易回升
* 但阈值太大 → **死区 timeout 又会回来**（削弱效果）

所以 Batch-4 的任务不是“证明 stuck detector 有用”（你们已经证明了），而是把它从“砍人斧”调成“手术刀”。

---

如果你愿意把 env 里 stuck detector 那段实际代码贴出来（哪怕 30 行），我还能进一步帮你检查两个最容易踩的点：

1. 计数器是否只在 dead_zone 内累计、退出后是否清零；
2. prev 值/计数器是否在 done mask 下正确 reset（并行 env 很容易串味）。
