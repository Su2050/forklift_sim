# S1.0Q Batch-2 复盘

> 日期：2026-02-13
> 分支：`feat/s1.0q`
> 底座：A1（dead_zone 惩罚, milestone_dead_zone_scale=0.0）
> Resume from：`model_3296.pt`（StageB3_full, 3296 iter）→ fine-tune 300 iter → `model_3595.pt`

---

## 1. 实验配置

| 实验 | 参数覆盖（在 A1 底座上） | 训练目录 | 说明 |
|------|------------------------|---------|------|
| A1 baseline | milestone_dead_zone_scale=0.0 | `2026-02-13_13-20-41` | Batch-1 产出，复用 |
| A2v2 窗口式撤退 | k_retreat=2.0 | `2026-02-13_18-12-35` | 8 步环形缓冲窗口净下降 |
| B1a' 温和底线 | ins_floor=0.2 | `2026-02-13_18-40-18` | 比 Batch-1 的 0.1 更温和 |
| C1 横向 shaping | k_lat_fine=0.8 | `2026-02-13_19-07-53` | 近场横向 delta shaping |

---

## 2. 评估结果统一对比表

评估条件：1024 envs × 2000 steps，每组约 3600-3700 episodes。

| 指标 | A1 baseline | A2v2 撤退窗口 | B1a' ins_floor=0.2 | C1 横向 shaping |
|------|:-----------:|:------------:|:------------------:|:--------------:|
| **success_rate_ep** | **84.72%** | 83.77% | 84.28% | 84.22% |
| **fail_step_share** | **35.04%** | 37.09% | 35.94% | 36.14% |
| **timeout_frac (P(ep_len==max))** | **14.36%** | 15.62% | 14.62% | 14.86% |
| **lateral_near_p90** | 0.328 m | 0.340 m | **0.306 m** | 0.332 m |
| **yaw_near_p90** | 6.56° | 6.83° | 6.63° | **6.55°** |
| P_ever_dead_zone | 13.19% | 14.16% | **11.63%** | 12.65% |
| P_dz_given_failed | 66.67% | 65.25% | **57.95%** | 63.62% |
| ep_reward_mean | 223.34 | 220.40 | 222.23 | 221.79 |
| ep_reward_p50 | 270.03 | 269.40 | 270.49 | 269.81 |
| mean_fail_ep_len | 1030 | 1049 | 1022 | 1033 |
| P_ever_near | 98.70% | 98.95% | 97.56% | 98.45% |
| P_ever_deep | 94.99% | 94.50% | 93.44% | 94.42% |
| mean_steps_first_near | 181.6 | 180.4 | 177.3 | 179.8 |
| n_episodes | 3692 | 3616 | 3722 | 3675 |

---

## 3. 逐实验分析

### 3.1 A2v2 — 窗口式撤退鼓励：失败

**结果**：全面退步。成功率 -0.95pp，fail_step_share +2.05pp，timeout +1.26pp，lateral p90 +12mm。

**原因分析**：

1. **信号被点亮但量级太弱**：冒烟测试确认 `r_retreat` 从 Batch-1 的完全为 0 变成了 0.0001 量级，但这个值约为总奖励的 0.005%，远不够驱动行为改变。
2. **根本问题不是"退不退"，而是"卡死不动"**：策略在死区里的典型模式是 insert_norm 基本不变（被碰撞卡住），即使用 8 步窗口，净变化仍然约为 0。窗口机制解决的是"逐步缓慢变化"的感知问题，但实际是"完全不变化"。
3. **信号冲突**：A1 惩罚策略"别在死区待着"，A2v2 奖励策略"在死区里退出来"。两个信号作用于同一状态区域，可能产生了梯度干扰。

**结论**：A2 路线"改题"而非"放弃"。撤退机制的前提（策略能动但方向错）不成立，但"脱困策略"本身有价值——应从 reward 维度的撤退激励升级为 **惩罚/终止卡死态**（dead-zone stuck detector），直接切断"跑满 max step 的吸血管"。

---

### 3.2 B1a' — 温和插入底线 (ins_floor=0.2)：**横向尾部与死区发生率上 Batch-2 最佳；但长尾 timeout 仍需专门机制**

**结果**：

- lateral p90：0.328 → **0.306 m**（-22mm，**Batch-2 改善最大**）
- 死区率：13.19% → **11.63%**（-1.56pp）
- P(dz|failed)：66.67% → **57.95%**（-8.72pp，失败 episode 中进过死区的比例显著下降）
- 成功率：84.72% → 84.28%（-0.44pp，在护栏范围内）
- P(ever_deep)：94.99% → 93.44%（-1.55pp，略有下降但远高于 90% 护栏）

**机制解读**：

将 Stage3 `phi_ins` 的底线从 0.4 降到 0.2，意味着当 `w_align3 ≈ 0`（横向严重偏离）时，策略从插入深度获得的"奖学金"从原来的 40% 降到 20%。这有效减弱了"横偏着也猛插"的动力，让策略更倾向于先修正横向再推进。

**为什么比 Batch-1 的 B1a (ins_floor=0.1) 好？**

Batch-1 的 B1a 太激进（底线降到 10%），导致策略在对齐过程中完全失去"先靠近再修正"的动力（"上游断粮"）。0.2 是一个更好的平衡点：既削弱了死区里的错误激励，又保留了足够的探索梯度。

**注意**：B1a' 的 fail_step_share（35.94%）和 timeout_frac（14.62%）并未优于 A1 alone（35.04% / 14.36%），差值可能在噪声范围内（~3700 episodes 下 0.5-1pp 级差异接近统计波动）。**这意味着 B1a' 打中了"横向误差"这条矛盾线，但"timeout 长尾"这条线完全没碰到**。

**结论**：**B1a' 在横向尾部和死区发生率维度上确认有效**，是最有价值的 Batch-2 发现。但 timeout 长尾（mean_fail_ep_len 仍在 1022）需要专门的 stuck detector 机制来处理。

---

### 3.3 C1 — 横向 delta shaping：边际有效

**结果**：

- yaw p90：6.56° → **6.55°**（微弱改善）
- lateral p90：0.328 → 0.332 m（+4mm，基本持平）
- 死区率：13.19% → 12.65%（-0.54pp，略有改善）
- 成功率：84.72% → 84.22%（-0.50pp）

**原因分析**：

1. **C1 是"精修"工具，不是"粗调"工具**：C1 的 delta shaping 在近场（insert_norm > 0.05）给横向收缩提供梯度，但当策略还在"横偏着猛插"时，这个信号无法扭转大趋势。
2. **被底层信号淹没**：`r_lat_fine` 的量级预期为总奖励的 ~3%，在当前仍有 40% ins_floor 底线的环境下，"横偏也能拿分"的正信号压过了 C1 的修正信号。
3. **可能在 B1a' 基础上更有效**：B1a' 已经削弱了横偏时的插入激励，C1 的修正信号在这个更"干净"的背景下可能更显著。

**结论**：**C1 单独不够强，但不放弃**。保留用于 B1a' 基础上的叠加测试（Batch-3）。

---

## 4. Batch-1 + Batch-2 全局进展

| 指标 | B0 原始 | A1 (B1结论) | B1a' (B2最佳) | 趋势 |
|------|---------|------------|-------------|------|
| success_rate_ep | 82.27% | 84.72% | 84.28% | +2.0pp |
| fail_step_share | 39.71% | 35.04% | 35.94% | -3.8pp |
| timeout_frac | 16.60% | 14.36% | 14.62% | -2.0pp |
| lateral_near_p90 | 0.344 m | 0.328 m | 0.306 m | -38mm |
| P_ever_dead_zone | 14.83% | 13.19% | 11.63% | -3.2pp |

从 B0 到现在，lateral p90 已从 0.344m 降到 0.306m，但距离 s1.0q 目标（0.25m）仍有 56mm 缺口。

---

## 5. 关键教训

1. **"信号点亮"不等于"行为改变"**：A2v2 的 r_retreat 从 0 变成 0.0001，技术上"被点亮"了，但 0.005% 的量级根本不可能跟总奖励的其他分量竞争。设计奖励信号时必须估算其占总奖励的比例，而不只是看"非零就好"。

2. **参数调节的甜蜜点很窄**：ins_floor 从 0.4（太松）→ 0.1（太紧）→ 0.2（刚好），说明奖励门控的激进程度需要精细调校。

3. **单因素实验的方法论依然有效**：B1a' 和 C1 的单因素结果清晰地指向了"B1a' 更强、C1 需要配合"的结论，为 Batch-3 的组合实验提供了明确的优先级排序。

---

## 6. Batch-3 方向

**新 baseline 固化**：A1+B1a' 不是新实验——Batch-2 的 B1a' 本身就是在 A1 底座上运行的，已验证的组合直接固化为 Batch-3 baseline。

基于以上分析，Batch-3 四组实验（均以 A1+B1a' 为底座）：

1. **+C1 横向精修叠加**（最高优先级）：B1a' 已削弱横偏激励，C1 在更干净的背景下应更显著
2. **+B1b 连续门控**：用 `w_lat_gate`（sigma=0.20）替代硬底线，看连续衰减是否更优
3. **ins_floor=0.15 扫参**：锁定甜蜜点斜率（0.1 太紧、0.2 有效，0.15 是否更好？）
4. **Dead-zone stuck detector**（专杀 timeout）：不看 action 幅度，只看状态变化——在死区内连续 30 步 insert/y_err 不变则强制终止

**A2 处置**：从"奖励退出来"改题为"惩罚/终止卡死态"（即上述第 4 条）

**搁置项**：
- C2 观测扩维（等组合实验确认 lateral p90 仍无法进一步下降后再考虑）
