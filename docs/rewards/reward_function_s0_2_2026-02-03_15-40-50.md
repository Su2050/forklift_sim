# 奖励函数 S0.2 模型文档

**模型信息：**
- **模型名称**: `exp_reward_s0_2`
- **训练时间**: 2026-02-03 15:40:50
- **模型文件**: `model_1999.pt` (第 1999/2000 轮)
- **训练时长**: 约 1 小时
- **总步数**: 131,072,000 steps

---

## 奖励机制概述

S0.2 是基于 S0 极简两阶段奖励方案的优化版本，主要调整：
- **`k_approach`**: 8.0 → **12.0**（增强接近奖励）
- **新增 `rew_time_penalty`**: **-0.001**/step（轻微时间压力）

### 核心设计思想

**对齐优先 + 安全站位 + 再插入抬升**

- 用软门控权重（`w_ready`, `w_lift`）实现自然过渡，不用复杂 phase 状态机
- 阶段 A：对齐为王（减小横向误差 + 偏航误差）
- 阶段 B：对齐好了才给接近/插入/抬升奖励

---

## 奖励参数配置

### 阶段阈值

| 参数 | 值 | 说明 |
|------|-----|------|
| `lat_ready_m` | 0.10 | 对齐就绪横向阈值（m） |
| `yaw_ready_deg` | 10.0 | 对齐就绪偏航阈值（度） |
| `d_safe_m` | 0.70 | 安全站位距离（m） |
| `insert_gate_norm` | 0.60 | 抬升门槛插入深度（归一化） |
| `insert_ramp_norm` | 0.10 | 抬升权重线性区间 |

### 奖励系数

| 参数 | 值 | 说明 |
|------|-----|------|
| `k_align` | 1.0 | 对齐进步奖励 |
| `k_approach` | **12.0** | 接近奖励（S0.2: 从 8.0 提高到 12.0） |
| `k_insert` | 10.0 | 插入进度奖励 |
| `k_lift` | 20.0 | 抬升奖励 |
| `k_close` | 2.0 | 没对齐就贴脸惩罚 |
| `k_wrongins` | 6.0 | 未对齐插入惩罚 |
| `rew_time_penalty` | **-0.001** | 时间惩罚（S0.1 新增） |
| `rew_success` | 100.0 | 成功基础奖励 |
| `rew_success_time` | 30.0 | 时间奖金 |

---

## 奖励计算公式

### 软门控权重

```python
# w_ready：对齐就绪权重（阶段 A → 阶段 B 的软开关）
w_lat = clamp(1.0 - y_err / lat_ready_m, 0, 1)
w_yaw = clamp(1.0 - yaw_err_deg / yaw_ready_deg, 0, 1)
w_ready = w_lat * w_yaw  # 很歪/很偏：≈0；对齐好：→1

# w_lift：抬升权重（插够深再抬）
w_lift = clamp((insert_norm - insert_gate_norm) / insert_ramp_norm, 0, 1)
```

### 对齐误差标量

```python
E_align = y_err / lat_ready_m + yaw_err_deg / yaw_ready_deg
```

### 奖励项（全部用增量）

```python
# 1. 对齐进步奖励（阶段 A 核心）
r_align = k_align * (last_E_align - E_align)

# 2. 安全距离惩罚（没对齐别贴脸）
pen_close = -k_close * (1.0 - w_ready) * relu(d_safe_m - dist_front)

# 3. 接近奖励（对齐好了才给）
r_approach = k_approach * w_ready * clamp(delta_dist, -0.05, 0.05)

# 4. 插入进度奖励（对齐好了才给）
r_insert = k_insert * w_ready * clamp(delta_insert, -0.03, 0.03)

# 5. 未对齐插入惩罚（防斜怼）
pen_wrong_insert = -k_wrongins * (1.0 - w_ready) * relu(delta_insert)

# 6. 抬升奖励（插够深才给）
r_lift = k_lift * w_lift * delta_lift

# 7. 时间惩罚（S0.1 新增）
rew_time = rew_time_penalty  # -0.001 per step

# 8. 成功奖励
time_ratio = episode_length / max_episode_length
time_bonus = rew_success_time * (1.0 - time_ratio)
success_total = rew_success + time_bonus
```

---

## 训练结果分析

### 最终性能（第 1999/2000 轮）

| 指标 | 初始值 | 最终值 | 变化 |
|------|--------|--------|------|
| **Mean reward** | 0.64 | **13.24** | +12.6 ✅ |
| **E_align_mean** | 2.81 | **0.56** | -2.25 ✅ |
| **w_ready_mean** | 0.04 | **0.66** | +0.62 ✅ |
| **frac_aligned** | 1.4% | **64.8%** | +63% ✅ |
| **lateral_mean** | 0.20m | **0.037m** | -0.16m ✅ |
| **yaw_deg_mean** | 7.7° | **1.86°** | -5.8° ✅ |
| **dist_front_mean** | 2.34m | **2.46m** | ⚠️ 没变 |
| **insert_norm_mean** | 0.0 | **0.0** | ⚠️ 没变 |
| **frac_inserted** | 0% | **0%** | ⚠️ 没变 |

### 奖励分量（最终）

| 分量 | 值 | 说明 |
|------|-----|------|
| `rew/r_align` | 0.0024 | 对齐进步奖励 |
| `rew/pen_close` | 0.0000 | 安全距离惩罚（未触发） |
| `rew/r_approach` | 0.0091 | 接近奖励（很小） |
| `rew/r_insert` | 0.0000 | 插入奖励（未触发） |
| `rew/pen_wrong_insert` | 0.0000 | 斜怼惩罚（未触发） |
| `rew/r_lift` | 0.0000 | 抬升奖励（未触发） |
| `rew/time_penalty` | -0.0010 | 时间惩罚 |
| `rew/success` | 0.0000 | 成功奖励（未成功） |

---

## 问题诊断

### ✅ 成功点

1. **对齐学习非常成功**
   - `E_align` 从 2.81 降到 0.56（降低 80%）
   - `w_ready` 从 0.04 升到 0.66（提升 16 倍）
   - 64.8% 的时间达到对齐标准
   - 横向误差 0.037m，偏航误差 1.86°（都很好）

2. **奖励稳步增长**
   - Mean reward 从 0.64 增长到 13.24

### ⚠️ 问题点

**策略陷入"远处对齐躺平"的局部最优**

1. **完全没有学会前进**
   - `dist_front_mean` 始终在 2.4-2.5m（初始位置）
   - `rew/r_approach` 只有 0.0091（太小，不足以推动前进）

2. **完全没有学会插入**
   - `insert_norm_mean` 始终为 0
   - `frac_inserted` 始终为 0%

### 根因分析

奖励结构导致策略选择"对齐但不前进"：

- ✅ `r_align` 在远处也能拿到（对齐变好就有奖励）
- ⚠️ `r_approach` 需要 `w_ready * delta_dist`，但策略发现"不动也不会被惩罚"
- ⚠️ `time_penalty` 只有 -0.001/step，不足以推动前进
- ⚠️ `k_approach=12.0` 仍然不够大，无法克服"不动"的惯性

---

## 改进建议

### 方案 A：大幅提高接近奖励

```python
k_approach: float = 20.0  # 从 12.0 提高到 20.0
```

### 方案 B：添加距离惩罚

```python
# 距离越远扣分越多
rew_distance: float = -0.5  # 每米 -0.5
distance_penalty = rew_distance * dist_front
```

### 方案 C：降低安全距离阈值

```python
d_safe_m: float = 0.50  # 从 0.70 降到 0.50
```

### 方案 D：增强时间压力

```python
rew_time_penalty: float = -0.005  # 从 -0.001 提高到 -0.005
```

---

## 模型文件位置

```
/home/uniubi/projects/forklift_sim/IsaacLab/logs/rsl_rl/forklift_pallet_insert_lift/2026-02-03_15-40-50_exp_reward_s0_2/model_1999.pt
```

---

## 相关文档

- [S0 基础方案](./reward_function_s0.md)
- [S0.1 版本](./reward_function_s0_1.md)（如果存在）
- [v4 方案](./reward_function_v4.md)

---

*文档创建时间：2026-02-03*
*模型训练完成时间：2026-02-03 15:40:50*
