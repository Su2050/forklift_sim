我把你的 **S1.0Q 实验计划**从“逻辑闭环 / 指标口径 / 奖励可被利用的漏洞 / 工程实现细节”四个角度过了一遍；整体来说，这份计划已经非常像一个能打仗的作战手册了：战略转向明确（先灭长尾失败），KPI 也终于和真实瓶颈对齐，而且 A→B→C 的递进结构很合理。

下面是我认为**最值得补强**的点（按“踩坑概率 × 影响程度”排序），每条都尽量给到“怎么改”的具体落点。

---

## 1) 最关键的一处：你要确保这些 KPI 在训练日志里“可计算且口径不漂”

你在 §1.1 把主 KPI 改成 per-episode 口径，这是正确方向；但你在 §1.3 又写“需要在训练日志新增 diag/fail_step_share（失败 episode 的 step 占比，近似）”。这里有个隐藏雷：

**单步日志本身并不知道“这一局最终是成功还是失败”，所以 fail_step_share 这种“按 episode 结果回溯分摊到 step”的指标，不能用普通的 per-step 累加得到（除非你做 episode 级汇总器）。**

### 建议补强（推荐二选一，别“近似”）

* **方案 A（推荐）：做 episode 级聚合器（在线）**

  * 给每个并行 env 维护 `ep_len[i]`、`ep_fail_len[i]`（先只记长度），以及 episode 结束时的 `is_success`。
  * episode done 时把长度累加到全局：

    * `total_steps += ep_len`
    * `fail_steps += ep_len if not success else 0`
  * 这样 `fail_step_share = fail_steps / total_steps` 是**严格定义**的，而不是“近似”。

* **方案 B：训练不加这些复杂统计，统一用离线诊断脚本算**

  * 每个 checkpoint 固定跑一套 eval（比如 deterministic，N episodes），产出你在 §1.1/§1.2 的 KPI 表。
  * 训练日志只保留你能稳定在线统计的量（均值、比例、计数），不要硬塞 p90 之类的分位数。

另外你写了 `diag/lateral_near_p90`、`diag/yaw_near_p90`。分位数在线统计不是做不了，但要么成本高，要么实现复杂（t-digest/直方图近似）。我建议把 p90 放到离线 eval 里更干净。

---

## 2) A2 撤退鼓励：你现在的 gating 逻辑会“奖励不到真正退出死区的那一步”

你 A2 的代码是按 **当前** `insert_norm` 与 `y_err` 来判定 `in_dead_zone`，然后只在 `in_dead_zone` 时给 `-k_retreat * delta_insert`。

这会导致一个非常微妙但很致命的现象：

* 如果策略从 `insert_norm=0.31` 退到 `0.29`（这一步是“成功退出死区”的关键一步）
* **在这一帧的“当前值”里它已经不在 dead_zone 了**
* 于是这一步的 `r_retreat` 变成 0 —— 你等于对“逃离死区的最后一脚”不给糖

### 你想要的其实是：**“上一帧在死区里”就奖励撤退**

改法很简单（思路级）：

* 用 `prev_in_dead_zone`（上一 step 的死区标记）来 gate 这一步的 retreat shaping
* 或者 gate 条件写成：`prev_insert_norm > thresh and prev_y_err > thresh`

同样地，A1 的 dead_zone 惩罚也可以考虑用“上一帧”或“soft gate”，否则在阈值附近会有不连续，可能诱发抖动/卡边界。

---

## 3) A1 + A2 的“奖励量级”建议再加一个硬护栏：每步惩罚/奖励 clamp

你现在 A1 的 per-step 惩罚大概是：
`-k_dead_zone*(insert-0.3)*(y-0.2)`，k=0.5。

这在数值上未必大（取决于你的 insert_norm/y_err 典型增量），但有个风险：**如果某些状态 y_err 很大或 insert_norm 被碰撞卡住不变，长期累加会把 episode return 拉得很负，PPO 可能出现“为了不亏，干脆不接近”的极端策略。**

你在风险表里已经意识到“不敢接近”风险（很好），但我建议你把缓解措施从“看护栏再调参”升级为**代码层面的硬护栏**：

* `pen_dead_zone = clamp(pen_dead_zone, min=-p, max=0)`
* `r_retreat = clamp(r_retreat, 0, +r)`

这样即便策略撞进奇怪状态，也不至于 reward 形状把训练炸穿。

---

## 4) B1 插入门控：你现在写了“两种方案”，但实验设计里只有一个 B1，容易混淆

B1 小节里你写了：

* 方案一：移除 0.4 底线（强硬）
* 方案二：ins_floor=0.1 + w_lat_gate（更平滑）

但在实验矩阵里它们都叫 B1。**这会造成复盘时非常痛苦**：到底“B1 成/败”指的是哪一个实现？

### 建议把 B1 拆成两个明确实验（强烈推荐）

* **B1a：只改 ins_floor（0.4→0.1），别额外乘 w_lat_gate**

  * 这一步最“正交”：你只是减少 misaligned 时候的保底插入奖学金
* **B1b：在 B1a 的基础上再加 w_lat_gate（或把 σ 设得更宽）**

  * 因为 `w_align3` 本身已经包含 y_err 的高斯门控了，再乘一个 `w_lat_gate` 等于“门控叠门控”，实际会变得更尖锐（有效 σ 会变小）

拆开后你会更容易判断：到底是“底线太高”还是“门控不够硬”在主导。

---

## 5) 你需要检查一个常被忽略的激励源：**milestone 奖励有没有绕过你的门控**

你在 A 组里诊断“死区里仍能拿插入进度奖学金”，并且从 `phi_ins` 的 0.4 底线入手，这是很对的。

但工程上经常出现的坑是：

> 你把 `phi_ins` 门控掉了，结果策略仍然靠 **r_milestone（里程碑奖励）** 在 misaligned 状态下“硬插拿分”。

计划里没有提 milestone 的门控策略，所以我建议在计划里补一句显式检查项：

* 若 milestone 触发条件仅依赖 insert_norm（或碰撞后的深度），那它也应当至少受一个对齐门控（y 或 yaw）影响
* 或者在 dead_zone 内禁用/衰减 milestone（最简单粗暴）

否则你 A/B 两组可能会出现“明明门控了，还是爱硬插”的怪相。

---

## 6) C2 观测扩维 warm-start：除了 actor 第一层，你还得把这些地方一起写进计划

你已经写了扩展 `actor.0.weight` 的示意，这很好。
但为了让 C2 真正可落地、少踩坑，我建议你在计划里补上三点“必须同步做”的工程清单：

1. **critic 的第一层也要扩维**（如果 actor/critic 分开网络）
2. 如果你用了 **obs running mean/std（观测归一化）**：

   * 新维度的统计量怎么初始化？（一般设 mean=0、var=1 或者重置整套统计）
3. **episode reset 时 prev 变量要按 done mask 清零/重置**

   * 你在 §7.2 提到新增 `_prev_insert_norm`, `_prev_phi_lat`，但没写“异步 reset 怎么处理”。
   * 1024 env 并行时，某些 env done、另一些没 done，必须按 mask 重置对应 index，否则 shaping 会跨 episode 串味。

把这三条写进计划，C2 的稳定性会高很多。

---

## 7) 实验排期：建议把 B1 提前并行（至少跑一个轻量版），因为它可能是“最便宜的上游止血”

你现在的顺序是：A1/A2 → A3 → B1/C1 → B2 → C2。这个顺序很合理，但我会建议加一个“小动作”：

* **在第一批并行里就加一个 B1a（只改 ins_floor，不加额外门控、不叠加 A 组）**

  * 原因：B1 是“防止进死区”的上游策略，可能比 A1/A2（进去后再自救）更省事
  * 即便 B1a 最终不够，它也能帮你更快判断“死区形成的根因到底是不是插入进度激励过强”

这样你的第一批会更像“同时打上游 + 下游”，信息增益更大。

---

## 8) 你现在的验收标准已经很棒，但还可以再补一条“防自欺”的对照项

你把主 KPI 放在 `fail_step_share`、`mean_fail_episode_len` 上很聪明，因为它们直击“吸血鬼长尾”。
为了避免出现“通过提前终止/逃避接近，让 fail episode 变短，从而 KPI 变好”的假胜利，我建议加一个对照指标：

* `P(episode ever_near)` / `P(episode ever_deep)`（per-episode）
* 以及 `mean_steps_to_first_near`（越小越好）

这样你能分辨两种完全不同的“fail 变短”：

* 好的：进死区后快速撤退重来/更快收敛
* 坏的：索性不靠近/不深插，早点结束

你虽然在护栏里放了 `deep_insert_frac`、`insert_norm_mean`，但“ever_deep”这种 per-episode 口径更不容易被 step 权重绕过。

---

# 最小改动版的“计划完善清单”

如果你只想做最少的修改，让计划从 90 分到 98 分，我建议就补这 5 项：

1. 明确：`fail_step_share`、p90 等 episode/分位数指标到底是 **在线聚合**还是 **离线 eval**（不要写“近似”）
2. 修正 A2 gating：用 **prev_in_dead_zone** 奖励“退出死区的那一步”
3. B1 拆成 B1a/B1b 两个实验，避免混淆
4. 检查并必要时门控 `r_milestone`，防止绕过插入门控
5. C2 warm start 补齐：critic 扩维 + obs 归一化统计 + done mask 重置 prev 变量

---

这套补强做完，你的 S1.0Q 就从“思路很对”升级成“复盘时不会被口径/实现细节背刺”的版本了。接下来真正有趣的部分会是：A 组把长尾砍短之后，B1/C1 到底谁是“让系统更聪明”的那把钥匙——这是会决定你后面要不要上 C2（观测扩维）的大分叉。
