# S1.0f 训练超参数与奖励机制详解

## 一、S1.0f PPO训练超参数

### 1.1 核心超参数配置

根据配置文件 `rsl_rl_ppo_cfg.py` 和训练日志，S1.0f的PPO超参数如下：

| 超参数 | S1.0f值 | 图片示例值 | 差异 | 说明 |
|--------|---------|-----------|------|------|
| **`num_envs`** | **1024** | ~128 | **8倍** | 并行环境数，S1.0f使用1024个并行环境 |
| **`num_steps_per_env`** | **64** | ~64 | **相同** | 每个环境收集的步数 |
| **`num_learning_epochs`** | **5** | ~10 | **减半** | 每次更新时的epoch数 |
| **`num_mini_batches`** | **4** | (隐含) | - | mini-batch数量 |
| **`clip_param` (ε)** | **0.2** | 0.2 | **相同** | PPO裁剪范围 |
| **`learning_rate`** | **3e-4** | (未明确) | - | 初始学习率 |
| **`gamma`** | **0.99** | (未明确) | - | 折扣因子 |
| **`lam` (GAE λ)** | **0.95** | (未明确) | - | GAE参数 |
| **`entropy_coef`** | **0.005** | (未明确) | - | 熵系数（S1.0降低到0.005） |
| **`init_noise_std`** | **3.0** | (未明确) | - | 初始探索噪声（S1.0提高到3.0） |
| **`max_iterations`** | **2000** | ~500 | **4倍** | 最大迭代次数 |

### 1.2 批次大小计算

**S1.0f实际配置**：

- 每批次总时间步 = `num_envs × num_steps_per_env = 1024 × 64 = 65,536 timesteps`
- 每个epoch的梯度步数 = `65,536 / num_mini_batches = 65,536 / 4 = 16,384 steps`
- 每次更新的总梯度步数 = `num_learning_epochs × 16,384 = 5 × 16,384 = 81,920 steps`

**图片示例**：

- 每批次总时间步 ≈ 2000 timesteps
- batch size = 64
- 每个epoch的梯度步数 = `2000 / 64 ≈ 31 steps`
- 每次更新的总梯度步数 = `10 × 31 ≈ 310 steps`（M≈300）

### 1.3 超参数差异分析

#### 1.3.1 `num_envs` (1024 vs ~128)

**差异**：S1.0f使用1024个并行环境，是示例的8倍

**影响**：

- **优点**：
  - 采样效率大幅提升：每批次收集65,536个经验样本（vs 示例的~2000）
  - 梯度估计更稳定：样本量大，方差更小
  - 训练速度更快：并行度高，GPU利用率高
- **缺点**：
  - GPU显存占用大：需要足够的显存支持1024个环境
  - 初始化时间更长：创建1024个环境需要更多时间

**对训练效果的影响**：

- ✅ **更快的经验收集**：每个迭代收集的经验量是示例的32倍
- ✅ **更稳定的梯度**：大样本量使梯度估计更准确
- ✅ **更好的探索**：1024个环境同时探索，覆盖更多状态空间

#### 1.3.2 `num_learning_epochs` (5 vs ~10)

**差异**：S1.0f使用5个epoch，示例使用10个epoch

**影响**：

- **优点**：
  - 训练速度更快：每个迭代的更新时间减半
  - 更好的on-policy特性：减少对旧数据的过拟合
  - 策略更新更保守：避免策略偏离旧策略太远
- **缺点**：
  - 数据利用率可能略低：每个样本只学习5次（vs 10次）

**对训练效果的影响**：

- ✅ **更符合PPO的on-policy假设**：减少对旧数据的重复学习
- ✅ **避免过拟合**：防止策略过度拟合当前批次数据
- ⚠️ **需要更多迭代**：可能需要更多迭代次数才能达到相同性能

#### 1.3.3 `clip_param` (0.2 vs 0.2)

**差异**：相同，都是0.2

**说明**：这是PPO算法的标准裁剪范围，将策略比率限制在[0.8, 1.2]区间内。

**对训练效果的影响**：

- ✅ **稳定的策略更新**：防止策略更新步长过大
- ✅ **避免性能崩溃**：限制策略变化幅度，保证训练稳定性

#### 1.3.4 `max_iterations` (2000 vs ~500)

**差异**：S1.0f使用2000次迭代，是示例的4倍

**影响**：

- **总经验量**：S1.0f总经验 = `2000 × 65,536 = 131,072,000 timesteps`（约1.31亿步）
- **示例总经验**：`500 × 2000 = 1,000,000 timesteps`（100万步）
- **S1.0f是示例的131倍经验量**

**对训练效果的影响**：

- ✅ **更充分的训练**：复杂任务需要更多经验才能收敛
- ✅ **更好的性能**：更多迭代允许策略逐步优化
- ⚠️ **训练时间更长**：需要更多计算资源

### 1.4 网络结构参数

| 参数 | S1.0f值 | 说明 |
|------|---------|------|
| `actor_hidden_dims` | [256, 256, 128] | Actor网络结构 |
| `critic_hidden_dims` | [256, 256, 128] | Critic网络结构 |
| `activation` | elu | 激活函数 |

## 二、S1.0f奖励机制详解

### 2.1 核心设计理念

S1.0f回归S0的**严格对齐机制**，同时保留S1.0e的改进：

1. **严格对齐阈值**：`lat_ready_m = 0.10m`, `yaw_ready_deg = 10.0°`（回归S0）
2. **乘法门控**：`w_ready = w_lat * w_yaw`（回归S0）
3. **接近奖励门控**：`r_approach = k_approach * w_ready * delta_dist`（回归S0）
4. **双重门控举升**：`w_lift = w_lift_base * w_ready`（保留S1.0e）
5. **绝对对齐惩罚**：`pen_align_abs`（保留S1.0c）
6. **距离自适应系数**：根据`dist_front`动态调整系数（保留S1.0）

### 2.2 对齐误差计算

#### 2.2.1 归一化综合误差 `E_align`

```python
# 横向误差（左右偏移）
y_err = |pallet_pos_y - robot_pos_y|  # 单位：米

# 偏航误差（朝向差）
yaw_err_deg = |pallet_yaw - robot_yaw|  # 单位：度

# 归一化综合误差
E_align = y_err / lat_ready_m + yaw_err_deg / yaw_ready_deg
# S1.0f: lat_ready_m = 0.10m, yaw_ready_deg = 10.0°
```

**数值示例**：

| 情况 | y_err | yaw_err | E_align | 评价 |
|------|-------|---------|---------|------|
| 完美对齐 | 0.00m | 0° | 0.00 | 理想状态 |
| 良好对齐 | 0.05m | 5° | 0.5 + 0.5 = **1.0** | 可接受 |
| 中等对齐 | 0.08m | 8° | 0.8 + 0.8 = **1.6** | 需改善 |
| 较差对齐 | 0.15m | 15° | 1.5 + 1.5 = **3.0** | 很差 |
| 初始状态 | ~0.20m | ~12° | 2.0 + 1.2 = **3.2** | 训练起点 |

### 2.3 软门控权重计算

#### 2.3.1 对齐就绪权重 `w_ready`（乘法门控）

```python
# 横向对齐权重
w_lat = clamp(1.0 - y_err / lat_ready_m, 0, 1)
# y_err=0时，w_lat=1；y_err≥0.10m时，w_lat=0

# 偏航对齐权重
w_yaw = clamp(1.0 - yaw_err_deg / yaw_ready_deg, 0, 1)
# yaw_err=0°时，w_yaw=1；yaw_err≥10°时，w_yaw=0

# 综合对齐就绪权重（乘法门控）
w_ready = w_lat * w_yaw
```

**乘法门控的含义**：

- ✅ **任一维度不达标，`w_ready`都会降低**
- ✅ **必须同时满足横向和偏航要求，`w_ready`才会接近1**

**数值示例**：

| y_err | yaw_err | w_lat | w_yaw | w_ready | 说明 |
|-------|---------|-------|-------|---------|------|
| 0.00m | 0° | 1.0 | 1.0 | **1.0** | 完美对齐 |
| 0.05m | 5° | 0.5 | 0.5 | **0.25** | 中等对齐 |
| 0.10m | 0° | 0.0 | 1.0 | **0.0** | 横向超标 |
| 0.00m | 10° | 1.0 | 0.0 | **0.0** | 偏航超标 |
| 0.08m | 8° | 0.2 | 0.2 | **0.04** | 较差对齐 |

#### 2.3.2 举升门控权重 `w_lift`（双重门控）

```python
# 基础举升权重（基于插入深度）
w_lift_base = clamp(
    (insert_norm - insert_gate_norm) / insert_ramp_norm,
    min=0.0, max=1.0
)
# insert_gate_norm = 0.60, insert_ramp_norm = 0.10
# 插入深度达到60%后开始线性增长，70%达到满权重

# 双重门控：必须同时满足插入深度和对齐质量
w_lift = w_lift_base * w_ready
```

**双重门控机制**：

- **第一重门控**：`w_lift_base`检查插入深度
- **第二重门控**：`w_ready`检查对齐质量
- **最终w_lift**：必须同时满足两个条件

### 2.4 距离自适应系数

S1.0f保留了S1.0的距离自适应机制，根据`dist_front`动态调整奖励系数：

#### 2.4.1 距离权重计算

```python
def smoothstep(x):
    x = torch.clamp(x, 0.0, 1.0)
    return x * x * (3.0 - 2.0 * x)

d_far   = 2.6   # 远端阈值
d_close = 1.1   # 近端阈值（S1.0f可能调整）
w_close = smoothstep((d_far - dist_front) / (d_far - d_close))  # 远=0 近=1
w_far   = 1.0 - w_close
```

#### 2.4.2 动态系数插值

5个关键系数按距离自动从"远处值"过渡到"近处值"：

```python
k_approach = k_app_far * w_far + k_app_close * w_close
k_align    = k_align_far * w_far + k_align_close * w_close
k_insert   = k_ins_far * w_far + k_ins_close * w_close
k_wrongins = k_wrong_far * w_far + k_wrong_close * w_close
k_premature = k_pre_far * w_far + k_pre_close * w_close
```

**S1.0f参数配置**（基于S1.0，可能微调）：

| 参数 | 远处值 (w_far=1) | 近处值 (w_close=1) | 设计意图 |
|------|-----------------|-------------------|----------|
| `k_approach` | **10.0** | 8.0 | 远处强拉前进，近处略降但保持正向动力 |
| `k_align` | 2.0 | **10.0** | 远处弱（允许探索），近处强（精对齐） |
| `k_insert` | 8.0 | **15.0** | 近处强化插入收益 |
| `k_wrongins` | 2.0 | **8.0** | 远处弱（允许探索），近处强（防斜怼） |
| `k_premature` | **12.0** | 5.0 | 远处极强（按死乱举升），近处放松（允许合理举升） |

### 2.5 奖励项详解

#### 2.5.1 对齐奖励（增量型 + 绝对惩罚）

```python
# 增量型对齐奖励（改善得分，恶化扣分）
r_align = k_align * delta_E_align
# delta_E_align = last_E_align - E_align，改善为正

# 绝对对齐惩罚（持续惩罚，只在近处生效）
pen_align_abs = -k_align_abs * w_close * clamp(E_align, 0.0, 2.0)
# k_align_abs = 0.10（S1.0d提升到0.10）
```

**设计原理**：

- **增量奖励**：只奖励改善，避免"站桩刷分"
- **绝对惩罚**：持续惩罚对齐误差，解决稳态梯度为零的问题

#### 2.5.2 接近奖励（对齐门控）

```python
# S1.0f：对齐好了才给接近奖励（回归S0）
r_approach = k_approach * w_ready * delta_dist
# delta_dist = last_dist - current_dist，接近为正
```

**设计原理**：

- **门控机制**：未对齐时（`w_ready ≈ 0`），接近奖励被抑制
- **强制序列**：策略必须先对齐，才能接近

#### 2.5.3 插入奖励（对齐门控）

```python
# S1.0f：对齐好了才给插入奖励
r_insert = k_insert * w_ready * clamp(progress, min=0.0)
# progress = delta_insert_depth，正向插入为正
```

**设计原理**：

- **门控机制**：未对齐时插入奖励被抑制
- **防止斜怼**：只有对齐质量足够好时，插入才能获得奖励

#### 2.5.4 举升奖励（双重门控）

```python
# S1.0f：双重门控举升奖励
r_lift = k_lift * w_lift * w_ready * clamp(delta_lift, min=0.0)
# w_lift = w_lift_base * w_ready（双重门控）
# k_lift = 20.0
```

**设计原理**：

- **双重门控**：必须同时满足插入深度和对齐质量
- **彻底切断斜怼路径**：防止通过物理撞入获得举升奖励

#### 2.5.5 远距离前进速度奖励

```python
# 只在远处生效，鼓励远处接近
r_forward = k_forward * w_far * clamp(v_xy_r_x, min=0.0)
# k_forward = 0.02
```

**设计原理**：

- **远处专用**：只在`w_far ≈ 1`时生效
- **明确信号**：让"向前开"在远处变成明确可学习的技能

#### 2.5.6 其他惩罚项

```python
# 空举惩罚（未插入时举升扣分）
pen_premature = -k_premature * (1.0 - w_lift) * clamp(delta_lift, min=0.0)

# 距离过远惩罚（防止远处躺平）
pen_dist_far = -k_dist_far * clamp(dist_front - 2.0, min=0.0)
# k_dist_far = 0.3

# 时间压力（每步固定惩罚）
rew_time_penalty = -0.003

# 动作平滑惩罚
rew_action_l2 = -0.01 * sum(actions^2)
```

### 2.6 成功判定标准

| 条件 | 阈值 | 说明 |
|------|------|------|
| 插入深度 | ≥ 2/3 × 2.16m = 1.44m | 货叉插入托盘的归一化深度 |
| 横向误差 | ≤ 0.03m | 严格对齐 |
| 偏航误差 | ≤ 3.0° | 严格对齐 |
| 举升高度 | ≥ 0.12m | 相对初始叉尖高度 |
| 保持时间 | ≥ 1.0s | 连续保持以上所有条件 |

**成功奖励**：

```python
success_reward = 100.0 + 30.0 * (1.0 - time_ratio)
# 基础奖励100.0 + 时间奖金（越早完成奖金越高，最高30.0）
```

## 三、超参数对训练效果的影响

### 3.1 PPO超参数影响

#### 3.1.1 `num_envs` (1024)

**对训练效果的影响**：

- ✅ **采样效率**：1024个环境并行采样，每批次收集65,536个样本，是示例的32倍
- ✅ **梯度稳定性**：大样本量使梯度估计更准确，训练更稳定
- ✅ **探索覆盖**：1024个环境同时探索，覆盖更多状态空间
- ⚠️ **显存需求**：需要足够的GPU显存（通常需要16GB+）

#### 3.1.2 `num_learning_epochs` (5)

**对训练效果的影响**：

- ✅ **On-policy特性**：减少对旧数据的重复学习，更符合PPO的on-policy假设
- ✅ **避免过拟合**：防止策略过度拟合当前批次数据
- ✅ **训练速度**：每个迭代的更新时间减半
- ⚠️ **数据利用率**：每个样本只学习5次，可能需要更多迭代

#### 3.1.3 `clip_param` (0.2)

**对训练效果的影响**：

- ✅ **稳定更新**：限制策略比率在[0.8, 1.2]区间，防止更新过大
- ✅ **避免崩溃**：防止策略性能突然下降
- ⚠️ **收敛速度**：较小的裁剪范围可能使收敛稍慢

#### 3.1.4 `entropy_coef` (0.005)

**对训练效果的影响**：

- ✅ **策略收敛**：较低的熵系数允许策略收敛到确定性策略
- ✅ **避免噪声爆炸**：防止noise_std持续上升（S0.7的经验教训）
- ⚠️ **探索不足**：如果设置过低，可能导致探索不足

#### 3.1.5 `init_noise_std` (3.0)

**对训练效果的影响**：

- ✅ **充分探索**：较大的初始噪声确保训练初期充分探索
- ✅ **避免过早收敛**：防止策略过早收敛到次优解
- ⚠️ **训练初期不稳定**：较大的噪声可能导致训练初期不稳定

### 3.2 奖励机制参数影响

#### 3.2.1 严格对齐阈值 (`lat_ready_m=0.10m`, `yaw_ready_deg=10.0°`)

**对训练效果的影响**：

- ✅ **精确对齐**：严格阈值强制策略学习精确对齐
- ✅ **高质量策略**：对齐质量直接影响后续插入和举升的成功率
- ⚠️ **训练难度**：严格阈值可能使训练初期困难，需要更多迭代

#### 3.2.2 乘法门控 `w_ready`

**对训练效果的影响**：

- ✅ **严格门控**：必须同时满足横向和偏航要求
- ✅ **防止妥协**：防止"一个好就能补偿另一个差"的情况
- ⚠️ **训练初期困难**：严格门控可能导致训练初期奖励信号很弱

#### 3.2.3 双重门控 `w_lift`

**对训练效果的影响**：

- ✅ **切断斜怼路径**：防止通过物理撞入获得举升奖励
- ✅ **强制正确序列**：必须先对齐→再插入→再举升
- ⚠️ **训练初期困难**：双重门控可能导致训练初期举升奖励很少

#### 3.2.4 距离自适应系数

**对训练效果的影响**：

- ✅ **自适应学习**：根据距离自动调整奖励系数，适应不同阶段的需求
- ✅ **避免平衡点**：动态系数避免在特定距离形成奖励平衡点
- ✅ **平滑过渡**：smoothstep函数实现平滑过渡，避免奖励跳变

## 四、总结

### 4.1 S1.0f超参数特点

1. **大规模并行**：1024个环境，大幅提升采样效率
2. **保守更新**：5个epoch，更好的on-policy特性
3. **严格对齐**：回归S0的严格对齐机制
4. **双重门控**：彻底切断斜怼+举升收益路径
5. **距离自适应**：根据距离动态调整奖励系数

### 4.2 与图片示例的对比

| 维度 | 图片示例 | S1.0f | 影响 |
|------|---------|-------|------|
| **并行环境数** | ~128 | 1024 | 采样效率提升8倍 |
| **每批次时间步** | ~2000 | 65,536 | 样本量提升32倍 |
| **更新epoch数** | ~10 | 5 | 更保守，更好的on-policy特性 |
| **总迭代次数** | ~500 | 2000 | 总经验量提升131倍 |
| **总经验时间步** | ~1M | ~131M | 复杂任务需要更多经验 |

### 4.3 预期训练效果

**短期（iter 0-500）**：

- `w_ready`开始上升（从~0.02 → >0.1）
- `yaw_deg`开始下降（从~12° → <10°）
- `dist_front`开始下降（从~3.0m → <2.5m）

**中期（iter 500-1500）**：

- `w_ready`持续上升（>0.3）
- `yaw_deg`持续下降（<5°）
- `insert_norm`开始上升（>0.2）
- `frac_success`开始提升（>1%）

**长期（iter 1500-2000）**：

- `w_ready` >0.5（良好对齐）
- `yaw_deg` <3°（接近目标）
- `frac_success` >10%（稳定成功策略）

---

*文档创建时间：2026-02-07*  
*基于S1.0f版本实现和PPO算法原理*
